<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en-us">
<head>
  <meta name="author" contents="Dennis Guse and Frank Haase">
  <meta name="date" content='2 June 2016'>
  <meta http-equiv="content-type" content="text/html; charset=utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0, maximum-scale=1">

  <link rel="icon" type="image/svg+xml" href="/images/steak-logo-centered.svg">
  <link rel="icon" type="image/x-icon" href="/images/steak-logo-centered.png">

  <link rel="alternate" type="application/rss+xml" title="RSS" href="/atom.xml">

  <title> The STEAK Project: Theory</title>
  <link rel="stylesheet" type="text/css" href="/css/style.css"></link>
  <style>
.initialAnimation {
position: fixed;
padding: 0;
margin: 0;
top: 0;
left:0;
width: 100%;
height: 100%;

background-image: url("/images/steak-logo.svg");
background-repeat:no-repeat;
background-position: center;
background-size:30%;
opacity:0;

animation-name: initialAnimation;
animation-duration: 4s;
animation-fill-mode: forwards;
}
@keyframes initialAnimation {
    0%   {opacity:1;background-color: red;}
    25%  {opacity:1;background-color: yellow;}
    50%  {opacity:1;background-color: blue;}
    99%  {opacity:0;background-color: green;height:100%;width:100%;}
    100% {height:0px;width:0px}
}
</style>
</head>

<body>
<div class="menu" id="menu">
  <div>
    <h1 class="project_title">
       <a href="/">
         <img src="/images/steak-logo.svg" alt="Logo" style="width:8em;top:10%" /><br />
         STEAK <p style="font-size: small">Spatial TelephonE conferencing for AsterisK</p>
       </a>
    </h1>
    <br />

    <nav><a href="/news#content">News</a> <small><a href="/atom.xml">(RSS Feed)</a></small></nav>
    <nav><a href="/project#content">The Project</a></nav>
    <nav><a href="/motivation#content">The Motivation</a></nav>
    <nav><a href="/advantages#content">The Advantages</a></nav>
    <nav><a href="/theory#content">The Theory of Spatial Rendering</a></nav>
    <nav><a href="/implementation#content">The Implementation</a></nav>
    <nav><a href="/demo#content">The Demo</a></nav>
    <nav><s><a href="/experiments#content">The Experiments</a></s></nav>
    <nav><a href="/faq#content">The FAQ</a></nav>
    <nav><a href="/about#content">About</a></nav>

    <br />
    <div>
      <s>&copy; 2016. All rights reserved.</s>&nbsp;
      <a href="https://creativecommons.org/"><img src="/images/license.svg" alt="CC-BY-SA 4.0" style="height: 1em"></a>
    </div>
  </div>
</div>

<div class="content" id="content">
  <div style="margin-bottom: 3em; font-size: 0.75em; text-align: center;"> --- 2 June 2016 --- </div>

  <h1 id="the-theory-of-spatial-rendering">The Theory of Spatial Rendering</h1>

<h2 id="sound-field">Sound field</h2>
<p>Positioning sound source(s) in an environment results in a <em>sound field</em>.
This sound field depends on the position of sound sources, their characteristics, and also reflections of sound waves.
The sound waves of individual sources can affect each other depending on their characteristic, position, and timing, i. e., elimate each other or amplify each other.</p>

<figure>
  <video src="/images/video_sound_field.mp4" controls="" loop="" style="width: 100%"></video>
  <figcaption><b>Video:</b> Sound field for two non-continuous sound sources.<br />
  <small>Data, and animation courtesy to <a href="https://github.com/hagenw/phd-thesis/blob/master/01_introduction/fig1_02/itd_ild.svg">Hagen Wierstorf</a>.</small></figcaption>
</figure>

<p>A sound field inherently contains spatial information about each sound source.
The spatial information can be derived to a certain degree by sampling the sound field using multiple microphones.
For constant sound sources signals, also one microphone can used while changing the position and, for unidirectional microphones, the direction of the microphone over time.
Evaluating the difference between the recorded signals, allows to estimate the position of individual sound sources.</p>

<h2 id="spatial-hearing">Spatial Hearing</h2>
<p>Humans are able to extract spatial information of a sound field using their ears.
A human ear is, in fact, a biologically microphone covered with a <a href="https://en.wikipedia.org/wiki/Auricle_(anatomy)">pinna</a>.
Using both ears, humans can derive spatial information of sound sources in their environment.
Depending on the position of a sound source, the sound waves reaching each ear are different (except if the source is directly in front or in the back of the head), as sound waves are traveling different paths to each ear.
These differences occur in timing (<em>Interaural Time Difference</em>, <a href="https://en.wikipedia.org/wiki/Sound_localization#Lateral_information_.28left.2C_ahead.2C_right.29">ITD</a>), level in terms of volume (<em>Interaural Level Difference</em>, <a href="https://en.wikipedia.org/wiki/Sound_localization#Lateral_information_.28left.2C_ahead.2C_right.29">ILD</a>), and frequency.
Differences in timing occur if the distance between the sound source and both ears are not equal.
A longer distance requires more time for sound waves to reach the ear and also reduces their energy (volume).
Differences in frequency for example occur due to objects that are accoustically not transparent.
If such objects are between the sound source and the ear, the sound waves cannot pass freely and this might block some frequency spectra partly or even completely (such as the head).
These characteristics might enable to derive the direction of a sound source relative the rotation of the head to a certain degree.
The distance of a sound source can be determined by the level of the signals reaching the ears (if a ground truth about a sound source is available), and by timing differences of reflections of the sound waves.</p>

<p>However, human hearing is not perfect.
For example, the angular resolution is not uniformly precise.
The highest resolution (horizontally) is available in front of the head (1-2 degrees) while to left and right the resolution decreases to up to 30 degrees.
In the vertical, the angular resolution is even lower.</p>

<h2 id="spatial-rendering-using-a-pair-of-headphones">Spatial Rendering using a Pair of Headphones</h2>
<p>The impression of spatiality of one or more sound source can be recreated using a pair of headphones.
One approach is to create <a href="https://en.wikipedia.org/wiki/Binaural_recording">binaural recordings</a> of a real sound enviroment.
Here, a <a href="https://en.wikipedia.org/wiki/Dummy_head_recording">dummy head</a>, representing a human head with two artificial ears, is used to record the sound field at the desired position (potentially with movements if desired).
Presenting the recorded signal of each ear binaurally via a pair of headphones to a person, enables this person to experience the sound field.
The listener experiences the sound field, as he would have been at the position of the dummy head includings its motion and head rotation.</p>

<figure>
  <video src="/images/video_impulse_response.mp4" controls="" loop="" style="width: 100%"></video>
  <figcaption><b>Video:</b> Impulse responses for both ears depending on head rotation for a sound source at 0&deg;, e.g., in front of the listener.<br />
  <small>Data, images, and animation courtesy to <a href="http://www.gnuplotting.org/animation-iii-video-revisited/">Gnuplotting.org</a> and <a href="https://github.com/hagenw/phd-thesis/blob/master/01_introduction/fig1_02/itd_ild.svg">Hagen Wierstorf</a>.</small></figcaption>
</figure>

<p>A spatial representation can also be <em>rendered</em>.
If this is done in real-time (here in terms of fast enough) then head rotation and position changes of the listener can be taken into account.
This is simulated by rendering the resulting signals of the left and right ear for all sound sources in the simulated sound field depending on their position, direction, and also reflections.
Basically, this adds the spatial cues to the signals (e. g., <a href="https://en.wikipedia.org/wiki/Sound_localization#Lateral_information_.28left.2C_ahead.2C_right.29">ITD</a> and <a href="https://en.wikipedia.org/wiki/Sound_localization#Lateral_information_.28left.2C_ahead.2C_right.29">ILD</a>), which can then be processed by the human hearing system.</p>

<p>Spatial rendering can be conducted using <a href="https://en.wikipedia.org/wiki/Convolution">convolution</a>.
Here, the signal of a sound source is convolved for each ear using a so-called <em>Head Related Transfer Function</em> (<a href="https://en.wikipedia.org/wiki/Head-related_transfer_function">HRTF</a>).
The <a href="https://en.wikipedia.org/wiki/Head-related_transfer_function">HRTF</a> describes the modifications applied to the signal from a sound source depending on its relative position to the ear.
The <a href="https://en.wikipedia.org/wiki/Head-related_transfer_function">HRTF</a> is actually the <em>Fourier Transform</em> of the <em>Head Related Impulse Response</em> (<a href="https://en.wikipedia.org/wiki/Head-related_transfer_function">HRIR</a>).
For convolution, the <em>Fourier Transform</em> of the source signal is multiplied with the <a href="https://en.wikipedia.org/wiki/Head-related_transfer_function">HRTF</a> and then the inverse Fourier transformation applied.
For a binaural representation, this is conducted for each ear individually.
Presenting these two signals at the same time to each ear correctly should result a spatial representation for a listener.
The distance of a sound source to the listener is simulated by adjusting the loudness (i. e.,  with an increasing distance a sound source gets less loud).
Multiple sound sources at the same time can be presented by mixing (i. e. adding) the convoluted signals of each ear.</p>

<h3 id="practical-limitations">Practical Limitations</h3>

<p>Providing a spatial representation using a pair of headphones has some practical issues.
This includes delay due to spatial rendering (problematic if head movements need to be taken into account), differences in audio equipment (e. g., the frequency response of headphones is not identical), and differences between humans and their individual learned spatial audio processing.
While delay can be reduced (e. g., <a href="https://en.wikipedia.org/wiki/Overlap%E2%80%93add_method">overlapp-add method</a>) and the differences in frequency response of headphones can be compensated to a certain degree, the signal modifications due to individual pinna shapes, ear distance, hairstyle, and also reflections of the human body can only be accounted for to a certain degree.
In fact, signal modifications due to differing pinna shapes can be accounted for by creating individualized <a href="https://en.wikipedia.org/wiki/Head-related_transfer_function">HRTFs</a>, often <a href="https://en.wikipedia.org/wiki/Head-related_transfer_function">HRTFs</a> are derived using a <em>standard</em> pinna are sufficiently to provide a satifisfying spatial representation.</p>

<p>Another practical issue occurs due to the fact that humans move their head to explore a their sound environment.
This allows to avoid front-back confusion, i. e., it is hard to determine if a sound source is in front or back of the listener as the signal modifications are very similar (same <a href="https://en.wikipedia.org/wiki/Sound_localization#Lateral_information_.28left.2C_ahead.2C_right.29">ILD</a> and <a href="https://en.wikipedia.org/wiki/Sound_localization#Lateral_information_.28left.2C_ahead.2C_right.29">ITD</a>).
In addition, also improved angular resolution in front of the listener can be exploited by rotating the head towards the sound source.
Moreover, also a listener might shift their position in a sound field to explore it.
The absence of the ability to move or rotate the head might limits the usefulness of a spatial representation.</p>

<h2 id="further-reading">Further Reading</h2>
<ul>
  <li>Blauert, <a href="https://mitpress.mit.edu/books/spatial-hearing"><em>Spatial Hearing</em></a>, MIT Press.</li>
</ul>


  <div style="text-align: center;" class="backtotop"><a href="#content">↩</a><a href="#menu">↩</a></div>
</div>

</body>
</html>
